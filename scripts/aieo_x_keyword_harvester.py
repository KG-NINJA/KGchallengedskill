import os
import json
import re
import time
import feedparser
from datetime import datetime, timedelta
from typing import List, Set, Dict, Optional, Tuple

# Nitter ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆè¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
NITTER_INSTANCES = [
    "https://nitter.net",
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.unixfox.eu",
    "https://nitter.1d4.us"
]

# ã‚ãªãŸã®Xãƒ¦ãƒ¼ã‚¶ãƒ¼å
X_USERNAME = "FuwaCocoOwnerKG"  # â† ã‚ãªãŸã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåã«å¤‰æ›´

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
MEMORY_FILE = "aieo_memory.json"
HARVESTED_KEYWORDS_FILE = "x_harvested_keywords.json"
CACHE_FILE = "x_posts_cache.json"
ERROR_LOG_FILE = "x_harvest_errors.log"

# è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’
REQUEST_TIMEOUT = 10  # ç§’
CACHE_EXPIRY_DAYS = 7


class XKeywordHarvester:
    """Xãƒã‚¹ãƒˆã‹ã‚‰AIEOç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆå …ç‰¢ç‰ˆï¼‰"""
    
    def __init__(self, username: str):
        self.username = username
        self.harvested = self._load_harvested()
        self.error_log = []
    
    def _load_harvested(self) -> Dict:
        """æ—¢ã«å–ã‚Šè¾¼ã¿æ¸ˆã¿ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(HARVESTED_KEYWORDS_FILE):
            try:
                with open(HARVESTED_KEYWORDS_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError as e:
                self._log_error(f"Failed to load harvested keywords: {e}")
                return self._init_harvested_structure()
        return self._init_harvested_structure()
    
    def _init_harvested_structure(self) -> Dict:
        """åˆæœŸåŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’è¿”ã™"""
        return {
            "keywords": [],
            "hashtags": [],
            "last_harvest": None,
            "total_posts_processed": 0,
            "harvest_history": []
        }
    
    def _save_harvested(self):
        """å–ã‚Šè¾¼ã¿æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜"""
        try:
            with open(HARVESTED_KEYWORDS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.harvested, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self._log_error(f"Failed to save harvested keywords: {e}")
    
    def _load_cache(self) -> Optional[List[Dict]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(CACHE_FILE):
            return None
        
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯
            cache_date = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_date > timedelta(days=CACHE_EXPIRY_DAYS):
                print("â° Cache expired")
                return None
            
            print(f"ğŸ“¦ Loaded {len(cache_data['posts'])} posts from cache")
            return cache_data['posts']
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            self._log_error(f"Failed to load cache: {e}")
            return None
    
    def _save_cache(self, posts: List[Dict]):
        """ãƒã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'posts': posts
            }
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Cached {len(posts)} posts")
        except Exception as e:
            self._log_error(f"Failed to save cache: {e}")
    
    def _log_error(self, message: str):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().isoformat()
        log_entry = f"[{timestamp}] {message}"
        self.error_log.append(log_entry)
        print(f"âŒ {message}")
        
        try:
            with open(ERROR_LOG_FILE, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')
        except:
            pass  # ãƒ­ã‚°è¨˜éŒ²å¤±æ•—ã¯ç„¡è¦–
    
    def fetch_recent_posts(self, days: int = 7) -> List[Dict]:
        """æœ€è¿‘ã®ãƒã‚¹ãƒˆã‚’RSSçµŒç”±ã§å–å¾—ï¼ˆå …ç‰¢ç‰ˆï¼‰"""
        
        # ã¾ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è©¦ã™
        cached_posts = self._load_cache()
        if cached_posts:
            return cached_posts
        
        print(f"ğŸ” Fetching posts from X (last {days} days)...")
        
        # å„Nitterã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è©¦ã™
        for attempt, instance in enumerate(NITTER_INSTANCES, 1):
            print(f"   Attempt {attempt}/{len(NITTER_INSTANCES)}: {instance}")
            
            posts = self._fetch_from_instance(instance, days)
            
            if posts:
                # æˆåŠŸã—ãŸã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è¿”ã™
                self._save_cache(posts)
                return posts
            
            # å¤±æ•—ã—ãŸã‚‰å°‘ã—å¾…ã¤
            if attempt < len(NITTER_INSTANCES):
                time.sleep(RETRY_DELAY)
        
        # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã€å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ã‚‚ä½¿ã†
        print("âš ï¸ All instances failed, trying expired cache...")
        expired_cache = self._load_expired_cache()
        if expired_cache:
            print(f"ğŸ“¦ Using expired cache ({len(expired_cache)} posts)")
            return expired_cache
        
        self._log_error("All fetch attempts failed, no cache available")
        return []
    
    def _fetch_from_instance(self, instance: str, days: int) -> List[Dict]:
        """ç‰¹å®šã®Nitterã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰å–å¾—"""
        try:
            rss_url = f"{instance}/{self.username}/rss"
            
            # User-Agentã‚’è¨­å®šã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿
            feed = feedparser.parse(rss_url, agent='Mozilla/5.0')
            
            if not feed.entries:
                return []
            
            posts = []
            cutoff_date = datetime.now() - timedelta(days=days)
            
            for entry in feed.entries:
                try:
                    # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    else:
                        # æ—¥ä»˜ãŒãªã„å ´åˆã¯ç¾åœ¨æ™‚åˆ»
                        pub_date = datetime.now()
                    
                    if pub_date < cutoff_date:
                        continue
                    
                    posts.append({
                        'title': entry.get('title', ''),
                        'content': entry.get('summary', ''),
                        'date': pub_date.isoformat(),
                        'link': entry.get('link', ''),
                        'source_instance': instance
                    })
                    
                except Exception as e:
                    self._log_error(f"Failed to parse entry: {e}")
                    continue
            
            if posts:
                print(f"   âœ… Found {len(posts)} posts")
                return posts
            
        except Exception as e:
            self._log_error(f"Failed to fetch from {instance}: {e}")
        
        return []
    
    def _load_expired_cache(self) -> Optional[List[Dict]]:
        """æœŸé™åˆ‡ã‚Œã§ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€ï¼ˆç·Šæ€¥ç”¨ï¼‰"""
        if not os.path.exists(CACHE_FILE):
            return None
        
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            return cache_data.get('posts', [])
        except:
            return None
    
    def extract_keywords(self, posts: List[Dict]) -> Set[str]:
        """ãƒã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        hashtag_pattern = re.compile(r'#(\w+)')
        proper_noun_pattern = re.compile(r'\b([A-Z][a-zA-Z]{2,}(?:[A-Z][a-z]+)*)\b')
        japanese_pattern = re.compile(r'[ã‚¡-ãƒ¶ãƒ¼]{2,}|[ä¸€-é¾¯]{2,}')
        url_pattern = re.compile(r'https?://\S+')
        
        for post in posts:
            # URLã‚’é™¤å»
            text = url_pattern.sub('', post['content'])
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡º
            hashtags = hashtag_pattern.findall(text)
            for tag in hashtags:
                if 2 < len(tag) < 30:  # é•·ã•åˆ¶é™
                    keywords.add(f"#{tag}")
                    if tag not in self.harvested["hashtags"]:
                        self.harvested["hashtags"].append(tag)
            
            # å›ºæœ‰åè©æŠ½å‡ºï¼ˆè‹±èªï¼‰
            proper_nouns = proper_noun_pattern.findall(text)
            for noun in proper_nouns:
                # é™¤å¤–ãƒªã‚¹ãƒˆ
                if noun in ['This', 'That', 'From', 'With', 'Have', 'Will', 'Been']:
                    continue
                if 3 < len(noun) < 30:
                    keywords.add(noun)
            
            # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            japanese_words = japanese_pattern.findall(text)
            for word in japanese_words:
                if 2 < len(word) < 20:
                    keywords.add(word)
        
        return keywords
    
    def filter_relevant_keywords(self, keywords: Set[str]) -> List[str]:
        """AIEOé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿"""
        
        # é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆä¸€èˆ¬çš„ã™ãã‚‹èªï¼‰
        exclude = {
            'RT', 'Twitter', 'Tweet', 'Follow', 'Like', 'Share', 'Click',
            'Link', 'Post', 'Reply', 'Retweet', 'Comment', 'Thread',
            'ãƒ„ã‚¤ãƒ¼ãƒˆ', 'ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ', 'ãƒ•ã‚©ãƒ­ãƒ¼', 'ã„ã„ã­', 'ãƒªãƒ—'
        }
        
        # å„ªå…ˆãƒªã‚¹ãƒˆï¼ˆå¿…ãšå–ã‚Šè¾¼ã‚€ï¼‰
        priority_keywords = {
            # å›ºæœ‰åè©
            'KGNINJA', 'AIEO', 'PsychoFrame', 'NOROSHI', 'FuwaCoco',
            'AutoKaggler', 'SceneMixer',
            
            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
            'Kaggle', 'GitHub', 'Fiverr',
            
            # æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
            'Python', 'JavaScript', 'n8n', 'Claude', 'ChatGPT',
            'Windsurf', 'Devin', 'OpenHands',
            
            # ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
            'AIEO', 'Beacon', 'Pulse', 'Resonance', 'Memory',
            
            # æˆæœ
            'OpenAI', 'Challenge', 'Hackathon', 'Competition'
        }
        
        filtered = []
        
        for kw in keywords:
            # é™¤å¤–ãƒªã‚¹ãƒˆãƒã‚§ãƒƒã‚¯
            if kw in exclude or kw.lower() in [e.lower() for e in exclude]:
                continue
            
            # å„ªå…ˆãƒªã‚¹ãƒˆã¯å¿…ãšè¿½åŠ 
            if kw in priority_keywords or kw.lower() in [p.lower() for p in priority_keywords]:
                filtered.append(kw)
                continue
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¯åŸºæœ¬çš„ã«å–ã‚Šè¾¼ã‚€
            if kw.startswith('#'):
                filtered.append(kw)
                continue
            
            # æ—¢å­˜ã®ãƒ¡ãƒ¢ãƒªã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if self._is_already_in_memory(kw):
                continue
            
            # é•·ã•ãƒã‚§ãƒƒã‚¯
            if 3 <= len(kw) <= 30:
                filtered.append(kw)
        
        # é‡è¤‡å‰Šé™¤ã—ã¦è¿”ã™
        return list(set(filtered))
    
    def _is_already_in_memory(self, keyword: str) -> bool:
        """æ—¢ã«ãƒ¡ãƒ¢ãƒªã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹"""
        existing = [k.lower() for k in self.harvested["keywords"]]
        return keyword.lower() in existing
    
    def update_memory(self, keywords: List[str], posts_count: int):
        """AIEOãƒ¡ãƒ¢ãƒªã«æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ """
        
        # ãƒ¡ãƒ¢ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        if not os.path.exists(MEMORY_FILE):
            print("âš ï¸ aieo_memory.json not found, will be created by memory engine")
            return
        
        try:
            with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
                memory = json.load(f)
        except json.JSONDecodeError as e:
            self._log_error(f"Failed to load memory: {e}")
            return
        
        # æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿æŠ½å‡º
        new_keywords = [kw for kw in keywords if kw not in self.harvested["keywords"]]
        
        if not new_keywords and not keywords:
            print("ğŸ“­ No new keywords to add")
            return
        
        # å±¥æ­´ã«è¿½åŠ 
        self.harvested["keywords"].extend(new_keywords)
        self.harvested["last_harvest"] = datetime.now().isoformat()
        self.harvested["total_posts_processed"] += posts_count
        
        # ä»Šå›ã®åç©«ã‚’è¨˜éŒ²
        harvest_record = {
            "timestamp": datetime.now().isoformat(),
            "posts_processed": posts_count,
            "new_keywords_count": len(new_keywords),
            "new_keywords": new_keywords[:10]  # æœ€åˆã®10ä»¶ã®ã¿
        }
        self.harvested["harvest_history"].append(harvest_record)
        
        # å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆæœ€æ–°30ä»¶ã®ã¿ä¿æŒï¼‰
        if len(self.harvested["harvest_history"]) > 30:
            self.harvested["harvest_history"] = self.harvested["harvest_history"][-30:]
        
        # Xç”±æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¦‚å¿µã‚’æ¤œç´¢
        x_concept_idx = None
        for i, concept in enumerate(memory["concepts"]):
            if concept["concept_id"] == "kg_x_keyword_evolution":
                x_concept_idx = i
                break
        
        # ãƒ¡ãƒ¢ãƒªæ¦‚å¿µã‚’æ›´æ–°
        concept_data = {
            "concept_id": "kg_x_keyword_evolution",
            "category": "dynamic_vocabulary",
            "attributes": {
                "harvested_keywords": list(set(self.harvested["keywords"]))[-50:],  # æœ€æ–°50ä»¶ï¼ˆé‡è¤‡æ’é™¤ï¼‰
                "total_unique_keywords": len(set(self.harvested["keywords"])),
                "recent_hashtags": list(set(self.harvested["hashtags"]))[-20:],
                "last_harvest": self.harvested["last_harvest"],
                "total_posts_analyzed": self.harvested["total_posts_processed"],
                "harvest_count": len(self.harvested["harvest_history"]),
                "source": "X (Twitter) timeline via RSS"
            },
            "confidence": min(0.95, 0.7 + (len(self.harvested["harvest_history"]) * 0.01)),
            "last_updated": datetime.now().isoformat()
        }
        
        if x_concept_idx is not None:
            memory["concepts"][x_concept_idx] = concept_data
            print(f"ğŸ“ Updated existing X keyword concept")
        else:
            memory["concepts"].append(concept_data)
            print(f"âœ¨ Created new X keyword concept")
        
        # ãƒ¡ãƒ¢ãƒªã‚’ä¿å­˜
        try:
            with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(memory, f, indent=2, ensure_ascii=False)
            print(f"âœ… Memory updated successfully")
        except Exception as e:
            self._log_error(f"Failed to save memory: {e}")
        
        # åç©«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self._save_harvested()
        
        # æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
        if new_keywords:
            print(f"\nğŸ¯ Added {len(new_keywords)} new keywords:")
            display_count = min(15, len(new_keywords))
            for kw in new_keywords[:display_count]:
                print(f"   - {kw}")
            if len(new_keywords) > display_count:
                print(f"   ... and {len(new_keywords) - display_count} more")
    
    def generate_summary(self) -> str:
        """åç©«ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        total_keywords = len(set(self.harvested["keywords"]))
        total_hashtags = len(set(self.harvested["hashtags"]))
        total_posts = self.harvested["total_posts_processed"]
        harvests = len(self.harvested["harvest_history"])
        
        summary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ¦ X KEYWORD HARVEST SUMMARY                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total Unique Keywords: {total_keywords:>4}                          â•‘
â•‘ Total Hashtags:        {total_hashtags:>4}                          â•‘
â•‘ Total Posts Analyzed:  {total_posts:>4}                          â•‘
â•‘ Harvest Count:         {harvests:>4}                          â•‘
â•‘ Last Harvest:          {self.harvested['last_harvest'][:19] if self.harvested['last_harvest'] else 'Never':>19} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return summary


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*60)
    print("ğŸ¦ AIEO X Keyword Harvester (Robust Edition)")
    print("="*60)
    
    # åˆæœŸåŒ–
    harvester = XKeywordHarvester(X_USERNAME)
    
    # ãƒã‚¹ãƒˆå–å¾—
    print(f"\nğŸ“¡ Fetching posts for @{X_USERNAME}...")
    posts = harvester.fetch_recent_posts(days=7)
    
    if not posts:
        print("\nâš ï¸ No posts found")
        print("   Possible reasons:")
        print("   - All Nitter instances are down")
        print("   - No recent posts in the last 7 days")
        print("   - Username might be incorrect")
        print("\nğŸ’¡ System will retry on next scheduled run")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãŒã‚ã‚Œã°è¡¨ç¤º
        if harvester.error_log:
            print("\nğŸ“‹ Error Log:")
            for log in harvester.error_log[-5:]:  # æœ€æ–°5ä»¶
                print(f"   {log}")
        
        return
    
    print(f"âœ… Successfully fetched {len(posts)} posts")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    print(f"\nğŸ” Extracting keywords...")
    raw_keywords = harvester.extract_keywords(posts)
    print(f"   Found {len(raw_keywords)} raw keywords")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print(f"\nğŸ¯ Filtering relevant keywords...")
    filtered = harvester.filter_relevant_keywords(raw_keywords)
    print(f"   Filtered to {len(filtered)} relevant keywords")
    
    # ãƒ¡ãƒ¢ãƒªæ›´æ–°
    print(f"\nğŸ§  Updating AIEO memory...")
    harvester.update_memory(filtered, len(posts))
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(harvester.generate_summary())
    
    print("="*60)
    print("âœ… X Keyword Harvest Complete")
    print("="*60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
